{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801e20fc",
   "metadata": {},
   "source": [
    "# Evaluation of LLC and ASP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36fe56",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e74d8",
   "metadata": {},
   "source": [
    "In this notebook, we perform a comparison between two algorithms. We present a brief summary of their parameters and outputs in a tabular format, which will serve as a reference throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61987348",
   "metadata": {},
   "source": [
    "### LLC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618ea17",
   "metadata": {},
   "source": [
    "The LLC algorithm comes in two versions: LLC-F, which includes additional faithfulness constraints, and LLC-NF, which does not. Both versions can be run bootstrapped, meaning that the algorithm is executed multiple times with different randomly sampled subsets of the data, and the results are averaged. They can also be run in their plain version. The parameters and outputs of the algorithms are listed below for quick reference, but a more comprehensive list can be found in the last section of the notebook.\n",
    "\n",
    "In the notebook \"llc_optuna_results.ipynb\", we evaluated the hyperparameters of the LLC algorithm, which included the following:\n",
    "\n",
    "* penalty type and regularization parameter ($\\lambda$):\n",
    "    * For LLC-NF, L1 and L2 penalty types had comparable performance regarding AUC ROC and accuracy. The optimal regularization parameter for both penalty types was between 0 and 0.1.\n",
    "    * For LLC-F, there was no significant difference regarding the penalty type. The optimal regularization parameter for both penalty types was between 0 and 0.1.\n",
    "    \n",
    "* significance level for CIT ($\\alpha_{llc}$):\n",
    "    * This parameter corresponds to the parameters p_dep and p_indep in the table below.\n",
    "    * Values in the range of 0.005 to 0.2 had comparable performance measured by accuracy. \n",
    "    * The AUC ROC prefers smaller $\\alpha$ values in that range. \n",
    "\n",
    "* z-score threshold when using the accuracy metric ($z_{llc}$):\n",
    "    * LLC-F optimal threshold values were in the range of 2-5.\n",
    "    * LLC-NF optimal threshold values were in the range of 5-20.\n",
    "    \n",
    "Based on the optimization results, we chose the **L1** penalty with a regularization parameter **$\\lambda = 0.05$**, **$\\alpha_{llc} = 0.05$** and $z_{llc}$ = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc04cad6",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">LLC plain algorithm input and outputs</p>\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\" rowspan=\"4\">parameters</td>\n",
    "      <td style=\"text-align:left\">penalty: str</td>\n",
    "      <td style=\"text-align:left\">Type of penalization/regularization used in the solving of the linear equation systems ('none', 'L0', 'L1' or 'L2').</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">lambda: int</td>\n",
    "      <td style=\"text-align:left\">Regularization parameter.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">rules: List[int]</td>\n",
    "      <td style=\"text-align:left\">Which of the which of the faithfulness rules (1,2,3) to apply.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">cond. indep. test</td>\n",
    "      <td style=\"text-align:left\">Constraints from faithfulness rules 2 and 3 are calculated using a conditional independence test and thresholds for dependence (p_dep) and independence (p_indep). A partial correlation test with p_dep = p_indep = 0.05 were originally used.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td  style=\"text-align:left\" rowspan=\"4\">output</td>\n",
    "      <td style=\"text-align:left\">B: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Estimate for the direct effects.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Ce: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Estimate of the covariance matrix. May contain NA if cov-condition is not satisfied for all pairs.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Bcon: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Conservative estimate of which elements of B are identified. See article for details.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Cecon: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Conservative estimate of which elements of Ce are identified.</td>\n",
    "    </tr>\n",
    " </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a018d",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">LLC bootstrap algorithm input and outputs</p>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">parameters <br> (additional <br> to plain)</td>\n",
    "    <td style=\"text-align:left\">n_bootstraps: int</td>\n",
    "    <td style=\"text-align:left\">Number times to run plain version of llc.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\" rowspan=\"4\">output <br> (additional <br> to plain)</td>\n",
    "    <td style=\"text-align:left\">Bsd: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Standard deviation of the estimated direct effects over the bootstraps.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Cesd: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Standard deviations of estimated elements of the covariance matrix.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Bz: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Z-scores for the elements of B. Absolute Z score is a measure of confidence on the existence of the edgs.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Cez: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Z-scores for the elements of Ce. Absolute Z score is a measure of confidence on the existence of the confounders.</td>\n",
    "  </tr>\n",
    "  <!--\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\" rowspan=\"2\">possible <br> usages of <br> output</td>\n",
    "    <td style=\"text-align:left\">predict {absent, present, [unknown]}</td>\n",
    "    <td style=\"text-align:left\">Classify as present if corresponding absolute value in Bz (or Cez for confounders) &gt; threshold. Classify as absent if corresponding absolute value in Bz (or Cez) &lt;= threshold. Use a threshold of 5 for both edges and confounders. Optional: classify as unknown if corresponding element in Bcon (or Cecon) is False, features classified as present or absent will have a value of True in Bcon (or Cecon).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">confidence value <br> (for ROC)</td>\n",
    "    <td style=\"text-align:left\">Absolute value of Bz (Cez for confounders).</td>\n",
    "  </tr>\n",
    "    -->\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1a1a1",
   "metadata": {},
   "source": [
    "### ASP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87acf893",
   "metadata": {},
   "source": [
    "The ASP algorithm has two different versions: ASP-d and ASP-s. ASP-d uses d-separation encoding, while ASP-s uses sigma-separation encoding. The parameters and output of the algorithm are listed in a table for reference.\n",
    "\n",
    "In the notebook \"asp_optuna_results.ipynb\", we evaluated the hyperparameters of the ASP algorithm, which included the following:\n",
    "\n",
    "* The significance leel for the conditional independence tests ($\\alpha_{asp}$):\n",
    "    * For both ASP-d and ASP-s values around 0.05 yielded optimal results.\n",
    "\n",
    "* The threshold $z_{asp}$ for transforming confidence values from running mode 3 into binary predictions:\n",
    "    * After optimization, we discovered that confidence optimization is not needed. Setting the confidence level to zero suffices in classifying zero-confidence features as present or absent.\n",
    "\n",
    "\n",
    "Based on the optimization results, we chose $\\alpha_{asp} = 0.05$ and $z_{asp}$ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58b727",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">ASP algorithm input and outputs</p>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">parameters</td>\n",
    "<td style=\"text-align:left\">cond. indep. test</td>\n",
    "<td style=\"text-align:left\">A conditional independence test that returns a p-value. A partial correlation test was used originally.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\"></td>\n",
    "<td style=\"text-align:left\">alpha: float</td>\n",
    "<td style=\"text-align:left\">Absolute threshold for independence. A value of 0.01 was used originally.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\"></td>\n",
    "<td style=\"text-align:left\">encoding: str</td>\n",
    "<td style=\"text-align:left\">encoding: str</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\"></td>\n",
    "<td style=\"text-align:left\">mode: int</td>\n",
    "<td style=\"text-align:left\">Which mode to query the ASP-solver (clingo) in.<br> Mode 1 runs clingo once.<br> Mode 2 runs clingo twice to compute the intersection and union of all optimal answer sets, specifying the markov equivalence class.<br> Mode 3 runs twice per feature, once with an additional 'pro' hard constraint enforcing the existence of a feature and once with an additional 'against' hard constraint enforcing the absence a feature</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">output</td>\n",
    "<td style=\"text-align:left\">mode 1</td>\n",
    "<td style=\"text-align:left\">Returns one optimal answer set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\"></td>\n",
    "<td style=\"text-align:left\">mode 2</td>\n",
    "<td style=\"text-align:left\">Returns a matrix calculated from the union and intersection sets containing predictions of present, absent or unknown.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\"></td>\n",
    "<td style=\"text-align:left\">mode 3</td>\n",
    "<td style=\"text-align:left\">Return a matrix of scores / confidence values calculated by taking the difference in loss in the 'against' run and the 'pro' run. A matrix is calculated from the scores matrix containing predictions of present, absent or unknown.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf396954",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8378c23",
   "metadata": {},
   "source": [
    "### Interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733febb",
   "metadata": {},
   "source": [
    "The performed interventins are by a dictionary, which can be seen in full below.\n",
    "\n",
    "The keys of the dictionary represent the intervention set identifiers and are integers with two digits, where the first digit indicates the number of variables involved in each intervention (either 1, 2 or 3), and the second digit indicates the number of interventions in the set (from 1 to 5).\n",
    "\n",
    "The values of the dictionary are lists of arrays, where each array describes an intervention. The arrays contain integers that represent the variables being intervened upon.\n",
    "\n",
    "Intervention set 0 is the purely observational data and corresponding value of the dictionary is [[]], indicating that no intervention is being performed.\n",
    "\n",
    "Intervention sets with two digits in the key starting with 1 represent interventions on a single variable. For example, intervention set 11 is a single intervention on variable 0, represented by the array [0].\n",
    "\n",
    "Intervention sets with two digits in the key starting with 2 represent interventions on two variables. For example, intervention set 23 has four interventions: the first is the observational data (represented by the empty array), and the subsequent interventions intervene on variables [0, 1], [1, 2], and [2, 3], respectively, represented by the arrays [0, 1], [1, 2], and [2, 3].\n",
    "\n",
    "Intervention sets with two digits in the key starting with 3 represent interventions on three variables. For example, intervention set 34 has four interventions: the first is the observational data (represented by the empty array), and the subsequent interventions intervene on variables [0, 1, 2], [1, 2, 3], and [2, 3, 4], respectively, represented by the arrays [0, 1, 2], [1, 2, 3], and [2, 3, 4].\n",
    "\n",
    "The maximum number of interventions in a single intervention set is 5, represented by the key 35, where the interventions are performed on variables [0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 0], and [4, 0, 1], respectively, represented by the arrays [0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 0], and [4, 0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555c745",
   "metadata": {},
   "source": [
    "The full dictionary is:\n",
    "* 0: [[]],\n",
    "* 11: [[], [0]],\n",
    "* 12: [[], [0], [1]],                \n",
    "* 13: [[], [0], [1], [2]],\n",
    "* 14: [[], [0], [1], [2], [3]],\n",
    "* 15: [[], [0], [1], [2], [3], [4]],\n",
    "* 21: [[], [0, 1]],\n",
    "* 22: [[], [0, 1], [1, 2]],\n",
    "* 23: [[], [0, 1], [1, 2], [2, 3]],\n",
    "* 24: [[], [0, 1], [1, 2], [2, 3], [3, 4]],\n",
    "* 25: [[], [0, 1], [1, 2], [2, 3], [3, 4], [4, 0]],\n",
    "* 31: [[], [0, 1, 2]],\n",
    "* 32: [[], [0, 1, 2], [1, 2, 3]],\n",
    "* 33: [[], [0, 1, 2], [1, 2, 3], [2, 3, 4]],\n",
    "* 34: [[], [0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 0]],\n",
    "* 35: [[], [0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 0], [4, 0, 1]]\n",
    "* 41: [[], [1,2,3,4]],\n",
    "* 42: [[], [1,2,3,4], [0,2,3,4]],\n",
    "* 43: [[], [1,2,3,4], [0,2,3,4], [0,1,3,4]],\n",
    "* 44: [[], [1,2,3,4], [0,2,3,4], [0,1,3,4], [0,1,2,4]],\n",
    "* 45: [[], [1,2,3,4], [0,2,3,4], [0,1,3,4], [0,1,2,4], [0,1,2,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ed565",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba11952",
   "metadata": {},
   "source": [
    "We create models using a unique random seed within each trial and maintain the same seed across different trials. To achieve this, we set the seed of the first model to 0, the second to 1, and so on. However, for this evaluation, we start the seed at 100 to avoid overlapping with the seeds 1-25 used in the hyperparameter evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4263f3e",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ea56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "from helpers import get_true, identification_stats, mask\n",
    "\n",
    "main_path = Path(Path.cwd().parent, 'simulations', 'data', 'hyperparameter_eval')\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200b2a",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0060e",
   "metadata": {},
   "source": [
    "### Loading LLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b01335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the hyperparameter_eval directory\n",
    "directory_path = main_path\n",
    "\n",
    "# Initialize empty lists for each column of the dataframe\n",
    "models = []\n",
    "experiments = []\n",
    "methods = []\n",
    "samplesizes = []\n",
    "B_values = []\n",
    "Ce_values = []\n",
    "Bcon_values = []\n",
    "Cecon_values = []\n",
    "Bz_values = []\n",
    "Cez_values = []\n",
    "\n",
    "# Iterate over the model folders\n",
    "for model_folder in os.listdir(directory_path):\n",
    "    if model_folder.startswith('model_'):\n",
    "        # Extract the model number from the folder name\n",
    "        model_num = int(model_folder.split('_')[1])\n",
    "        \n",
    "        # Iterate over the sample size folders within the model folder\n",
    "        for samplesize_folder in os.listdir(os.path.join(directory_path, model_folder)):\n",
    "            if samplesize_folder.startswith('samplesize_'):\n",
    "                # Extract the sample size from the folder name\n",
    "                if samplesize_folder == 'samplesize_inf':\n",
    "                    samplesize = np.inf\n",
    "                else:\n",
    "                    samplesize = int(samplesize_folder.split('_')[1])\n",
    "                \n",
    "                # Iterate over the experiment folders within the sample size folder\n",
    "                for experiment_folder in os.listdir(os.path.join(directory_path, model_folder, samplesize_folder)):\n",
    "                    \n",
    "                    if experiment_folder.startswith('experiment_'):\n",
    "                        # Extract the experiment number from the folder name\n",
    "                        experiment_num = int(experiment_folder.split('_')[1])\n",
    "                        \n",
    "                        # Iterate over the method folders within the experiment folder\n",
    "                        for method_folder in os.listdir(os.path.join(directory_path, model_folder, samplesize_folder, experiment_folder)):\n",
    "                            \n",
    "                            if method_folder.startswith('llc'):\n",
    "                                method = method_folder\n",
    "                                \n",
    "                                # Read the pickled dictionary from the llc_out.pkl file\n",
    "                                pkl_file_path = os.path.join(directory_path, model_folder, samplesize_folder, experiment_folder, method_folder, 'llc_out.pkl')\n",
    "                                with open(pkl_file_path, 'rb') as f:\n",
    "                                    pkl_dict = pickle.load(f)\n",
    "                                \n",
    "                                # Extract the B, Ce, Bez, and Cez values from the pickled dictionary\n",
    "                                B = pkl_dict['B']\n",
    "                                Ce = pkl_dict['Ce']\n",
    "                                Bcon = pkl_dict['Bcon']\n",
    "                                Cecon = pkl_dict['Cecon']\n",
    "                                Bz = pkl_dict.get('Bz', np.nan) if samplesize != np.inf else np.nan\n",
    "                                Cez = pkl_dict.get('Cez', np.nan) if samplesize != np.inf else np.nan\n",
    "                                \n",
    "                                # Append the values to the corresponding lists\n",
    "                                models.append(model_num)\n",
    "                                experiments.append(experiment_num)\n",
    "                                methods.append(method)\n",
    "                                samplesizes.append(samplesize)\n",
    "                                B_values.append(B)\n",
    "                                Ce_values.append(Ce)\n",
    "                                Bcon_values.append(Bcon)\n",
    "                                Cecon_values.append(Cecon)\n",
    "                                Bz_values.append(Bz)\n",
    "                                Cez_values.append(Cez)\n",
    "                            \n",
    "                            \n",
    "\n",
    "# Create the pandas dataframe from the lists\n",
    "df_llc = pd.DataFrame({\n",
    "    'model': models,\n",
    "    'experiment': experiments,\n",
    "    'method': methods,\n",
    "    'samplesize': samplesizes,\n",
    "    'B': B_values,\n",
    "    'Ce': Ce_values,\n",
    "    'Bcon': Bcon_values,\n",
    "    'Cecon': Cecon_values,\n",
    "    'Bz': Bz_values,\n",
    "    'Cez': Cez_values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33bb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306e912",
   "metadata": {},
   "source": [
    "## Loading ASP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06815eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the hyperparameter_eval directory\n",
    "directory_path = main_path\n",
    "\n",
    "# Initialize empty lists for each column of the dataframe\n",
    "models = []\n",
    "experiments = []\n",
    "methods = []\n",
    "samplesizes = []\n",
    "edges_scores = []\n",
    "edges_preds = []\n",
    "confs_scores = [] \n",
    "confs_preds = []\n",
    "\n",
    "# Iterate over the model folders\n",
    "for model_folder in os.listdir(directory_path):\n",
    "    if model_folder.startswith('model_'):\n",
    "        # Extract the model number from the folder name\n",
    "        model_num = int(model_folder.split('_')[1])\n",
    "        \n",
    "        # Iterate over the sample size folders within the model folder\n",
    "        for samplesize_folder in os.listdir(os.path.join(directory_path, model_folder)):\n",
    "            if samplesize_folder.startswith('samplesize_'):\n",
    "                # Extract the sample size from the folder name\n",
    "                if samplesize_folder == 'samplesize_inf':\n",
    "                    samplesize = np.inf\n",
    "                else:\n",
    "                    samplesize = int(samplesize_folder.split('_')[1])\n",
    "                \n",
    "                # Iterate over the experiment folders within the sample size folder\n",
    "                for experiment_folder in os.listdir(os.path.join(directory_path, model_folder, samplesize_folder)):\n",
    "                    \n",
    "                    if experiment_folder.startswith('experiment_'):\n",
    "                        # Extract the experiment number from the folder name\n",
    "                        experiment_num = int(experiment_folder.split('_')[1])\n",
    "                        \n",
    "                        # Iterate over the method folders within the experiment folder\n",
    "                        for method_folder in os.listdir(os.path.join(directory_path, model_folder, samplesize_folder, experiment_folder)):\n",
    "                            \n",
    "                            if method_folder.startswith('asp'):\n",
    "                                \n",
    "                                for sep in ['s', 'd']:\n",
    "                                    method = f'asp_{sep}'\n",
    "                                    tmp_path = os.path.join(directory_path, model_folder, samplesize_folder, experiment_folder, method_folder)\n",
    "                                    \n",
    "                                    # Read the csv files containing the predictions\n",
    "                                    edges_score = pd.read_csv(os.path.join(tmp_path, f'edges_score_{sep}_sep.csv'), header=None)\n",
    "                                    edges_pred = pd.read_csv(os.path.join(tmp_path, f'edges_pred_{sep}_sep.csv'), header=None)\n",
    "                                    confs_score = pd.read_csv(os.path.join(tmp_path, f'confs_score_{sep}_sep.csv'), header=None)\n",
    "                                    confs_pred = pd.read_csv(os.path.join(tmp_path, f'confs_pred_{sep}_sep.csv'), header=None)\n",
    "                                \n",
    "                                    # Append the values to the corresponding lists\n",
    "                                    models.append(model_num)\n",
    "                                    experiments.append(experiment_num)\n",
    "                                    methods.append(method)\n",
    "                                    samplesizes.append(samplesize)\n",
    "                                    edges_scores.append(edges_score.values)\n",
    "                                    edges_preds.append(edges_pred.values)\n",
    "                                    confs_scores.append(confs_score.values)\n",
    "                                    confs_preds.append(confs_pred.values)\n",
    "                            \n",
    "                            \n",
    "\n",
    "# Create the pandas dataframe from the lists\n",
    "df_asp = pd.DataFrame({\n",
    "    'model': models,\n",
    "    'experiment': experiments,\n",
    "    'method': methods,\n",
    "    'samplesize': samplesizes,\n",
    "    'edges_score': edges_scores,\n",
    "    'edges_pred': edges_preds,\n",
    "    'confs_score': confs_scores,\n",
    "    'confs_pred': confs_preds\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364eae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_asp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e48cc1",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916538e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data of true models\n",
    "pairs = get_true(filepath=main_path)\n",
    "df_models = pd.DataFrame(data=[[100+i, edge, conf] for i, (edge, conf) in enumerate(pairs)],\n",
    "                  columns=['model', 'edges', 'confs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicate models\n",
    "print(f'Checking {df_models.shape[0]} models for duplicates ..')\n",
    "\n",
    "duplicates = 0\n",
    "for a, b in itertools.combinations(range(len(df_models['edges'])), 2):\n",
    "    if np.array_equal(df_models['edges'][a], df_models['edges'][b]):\n",
    "        duplicates += 1\n",
    "\n",
    "if duplicates > 0:\n",
    "    print('There are duplicate models!!')\n",
    "else:\n",
    "    print('There are no duplicate models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number of directed edges\n",
    "avg_directed_edges = df_models['edges'].sum().sum() / (df_models.shape[0])\n",
    "\n",
    "# Calculate the average number of bidirected edges\n",
    "avg_bidirected_edges = df_models['confs'].sum().sum() / (df_models.shape[0] * 2)\n",
    "\n",
    "print(\"Average number of directed edges:\", avg_directed_edges)\n",
    "print(\"Average number of bidirected edges:\", avg_bidirected_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average\n",
    "acc_weak_classifier = (30 - (2 + avg_directed_edges)) / 30\n",
    "print(\"Accuracy of weak classifier that classifies everything as absent: \", acc_weak_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd6bff",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184bd80",
   "metadata": {},
   "source": [
    "### ASP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d9974",
   "metadata": {},
   "source": [
    "The ASP algorithm produces confidence scores for each feature, where a positive score indicates the presence of the feature, a negative score indicates its absence, and a score of zero indicates that the algorithm is unsure. \n",
    "The accuracy is the defined as (number of correct predictions) / (total number of predictions). To calculate it in this case, we need to define what constitutes a correct prediction. As a reminder, the confusion matrix for this type of prediction is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bf336",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">Confusion matrix.</p>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th rowspan=\"2\"></th>\n",
    "      <th rowspan=\"2\"></th>\n",
    "      <th colspan=\"2\">Actual condition</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Present</th>\n",
    "      <th>Absent</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th rowspan=\"3\">Predicted<br>condition</th>\n",
    "      <td>Positive</td>\n",
    "      <td>true positive (tp)</td>\n",
    "      <td>false positive (fp)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Unknown</td>\n",
    "      <td>positive unknown (pu)</td>\n",
    "      <td>negative unknown (nu)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Negative</td>\n",
    "      <td>false negative (fn)</td>\n",
    "      <td>true negative (tn)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d5766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix.\n",
    "def identification_stats_wrapper_asp(row, df_models, type='edge'):\n",
    "    A = df_models.loc[df_models['model'] == row['model'], f'{type}s'].iloc[0]\n",
    "    B = row[f'{type}s_pred']\n",
    "    return identification_stats(A, B, type)\n",
    "\n",
    "df_asp[['tp_edge', 'fp_edge', 'pu_edge', 'nu_edge', 'fn_edge', 'tn_edge']] = df_asp.apply(identification_stats_wrapper_asp, args=(df_models,'edge',), axis=1, result_type='expand')\n",
    "df_asp[['tp_conf', 'fp_conf', 'pu_conf', 'nu_conf', 'fn_conf', 'tn_conf']] = df_asp.apply(identification_stats_wrapper_asp, args=(df_models, 'conf'), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24187863",
   "metadata": {},
   "source": [
    "We can convert the ternary predictions into binary ones by treating \"unknown\" predictions \"absent\". If the default label class is \"absent\" then the number of correct predictions is (tp + tn + nu) and the total number of predictions is (tp + fp + tn + fn + pu + nu). Note that This approach is biased and dependent on the graph density. See the appendix for other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for edges if unknowns are classified as positive / absent\n",
    "df_asp['acc_absent_edge'] = (df_asp['tp_edge'] + df_asp['tn_edge'] + df_asp['nu_edge']) / (df_asp['tp_edge'] + df_asp['tn_edge'] + df_asp['fp_edge'] + df_asp['fn_edge'] + df_asp['pu_edge'] + df_asp['nu_edge'])\n",
    "# accuracy for confounders if unknowns are classified as positive / absent\n",
    "df_asp['acc_absent_conf'] = (df_asp['tp_conf'] + df_asp['tn_conf'] + df_asp['nu_conf']) / (df_asp['tp_conf'] + df_asp['tn_conf'] + df_asp['fp_conf'] + df_asp['fn_conf'] + df_asp['pu_conf'] + df_asp['nu_conf'])\n",
    "# mean accuracy for edges and confounders if unknowns are classified negative / absent\n",
    "df_asp['acc_absent'] = (df_asp['acc_absent_edge'] + df_asp['acc_absent_conf']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3f353",
   "metadata": {},
   "source": [
    "### LLC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a92fc",
   "metadata": {},
   "source": [
    "To calculate the accuracy, we use confidence values in Bz and Cez (for finite samples) or B and Ce (for infinite samples) and apply a threshold to classify them as either \"present\" or \"absent.\" This approach results in a 2x2 confusion matrix, and we can calculate the accuracy as we would in any standard binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix.\n",
    "def identification_stats_wrapper_llc(row, df_models, type='edge'):\n",
    "    A = df_models.loc[df_models['model'] == row['model'], f'{type}s'].iloc[0]\n",
    "    \n",
    "    if isinstance(row['Bz'], np.ndarray):\n",
    "        if type == 'edge':\n",
    "            B = mask(np.abs(row['Bz']), threshold=5)\n",
    "        if type == 'conf':\n",
    "            B = mask(np.abs(row['Cez']), threshold=5)  \n",
    "    else:\n",
    "        if type == 'edge':\n",
    "            B = mask(np.abs(row['B']), threshold=0.1)\n",
    "        if type == 'conf':\n",
    "            B = mask(np.abs(row['Ce']), threshold=0.1)  \n",
    "\n",
    "    return identification_stats(A, B, type)\n",
    "\n",
    "df_llc[['tp_edge_binary', 'fp_edge_binary', 'pu_edge_binary', 'nu_edge_binary', 'fn_edge_binary', 'tn_edge_binary']] = df_llc.apply(identification_stats_wrapper_llc, args=(df_models,'edge',), axis=1, result_type='expand')\n",
    "df_llc[['tp_conf_binary', 'fp_conf_binary', 'pu_conf_binary', 'nu_conf_binary', 'fn_conf_binary', 'tn_conf_binary']] = df_llc.apply(identification_stats_wrapper_llc, args=(df_models,'conf',), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12559f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entries \"pu\" and \"nu\" of the confusion matrix should be zero.\n",
    "assert (df_llc[['pu_edge_binary', 'nu_edge_binary', 'pu_conf_binary', 'nu_conf_binary']] == 0).all(axis=1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for edges using only confidence values\n",
    "df_llc['acc_edge_4'] = (df_llc['tp_edge_binary'] + df_llc['tn_edge_binary']) / (df_llc['tp_edge_binary'] + df_llc['tn_edge_binary'] + df_llc['fp_edge_binary'] + df_llc['fn_edge_binary'])\n",
    "# accuracy for confounders if unknowns predictions as incorrect\n",
    "df_llc['acc_conf_4'] = (df_llc['tp_conf_binary'] + df_llc['tn_conf_binary']) / (df_llc['tp_conf_binary'] + df_llc['tn_conf_binary'] + df_llc['fp_conf_binary'] + df_llc['fn_conf_binary'])\n",
    "# mean accuracy for edges and confounders using only confidence value\n",
    "df_llc['acc_4'] = (df_llc['acc_edge_4'] + df_llc['acc_conf_4']) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5938c29",
   "metadata": {},
   "source": [
    "## AUC ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc42a05",
   "metadata": {},
   "source": [
    "The next section presents a set of ROC plots that show the performance of the models across different experiments and sample sizes. To calculate the ROC curves, the confidence values for the edges and confounders were used. The approach taken involves combining the edges and confounders of all 150 models into a single ROC curve using the confidence values, which is then used to calculate the AUC ROC. Additionally, one AUC ROC value is calculated for each experiment x samplesize. The confidence values used vary depending on the type of model being analyzed. For the LLC method with finite samples, the absolute values of Bz and Cez are used, while for the LLC method with infinite samples, the absolute values of B and Ce are used. Finally, for the ASP method, the score matrix of edges and confounders is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac4578",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate AUC ROC for LLC\n",
    "\n",
    "# Initialize results dataframe\n",
    "auc_llc = pd.DataFrame(columns=['experiment', 'samplesize', 'method', 'model', 'auc_roc'])\n",
    "df_list = [] # to hold the dataframes for each iteration\n",
    "\n",
    "# Group df_llc by experiment, samplesize, and method\n",
    "groups = df_llc.groupby(['experiment', 'samplesize', 'method'])\n",
    "\n",
    "model_numbers = df_models['model'].unique()\n",
    "\n",
    "# Iterate over each group\n",
    "for index, group in groups:\n",
    "    experiment, samplesize, method = index\n",
    "    scores, labels = [], []\n",
    "\n",
    "    for model_number in model_numbers:\n",
    "        model = df_models[df_models['model'] == model_number]\n",
    "        b_col = 'Bz' if samplesize != np.inf else 'B'\n",
    "        ce_col = 'Cez' if samplesize != np.inf else 'Ce'\n",
    "        B, Ce = group.loc[group['model'] == model_number, b_col].abs().values[0], \\\n",
    "                 group.loc[group['model'] == model_number, ce_col].abs().values[0]\n",
    "        \n",
    "        for row in range(B.shape[0]):\n",
    "            for col in range(B.shape[1]):\n",
    "                if row != col:\n",
    "                    true_label_edge = model['edges'].values[0][row, col]\n",
    "                    true_label_conf = model['confs'].values[0][row, col]\n",
    "                    labels.append(true_label_edge)\n",
    "                    labels.append(true_label_conf)\n",
    "                    scores.append(B[row, col])\n",
    "                    scores.append(Ce[row, col])\n",
    "\n",
    "    # Calculate AUC ROC and add to results dataframe\n",
    "    auc_roc = roc_auc_score(labels, scores)\n",
    "    df_list.append(pd.DataFrame({'experiment': experiment, 'samplesize': samplesize, 'method': method,\n",
    "                              'model': model_number, 'auc_roc': auc_roc}, index=[0]))\n",
    "\n",
    "# Concatenate dataframes and reset index\n",
    "auc_llc = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8280381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate AUC ROC for ASP\n",
    "\n",
    "# Initialize results dataframe\n",
    "auc_asp = pd.DataFrame(columns=['experiment', 'samplesize', 'method', 'model', 'auc_roc'])\n",
    "df_list = [] # to hold the dataframes for each iteration\n",
    "\n",
    "# Group df_llc by experiment, samplesize, and method\n",
    "groups = df_asp.groupby(['experiment', 'samplesize', 'method'])\n",
    "\n",
    "model_numbers = df_models['model'].unique()\n",
    "\n",
    "# Iterate over each group\n",
    "for index, group in groups:\n",
    "    experiment, samplesize, method = index\n",
    "    scores, labels = [], []\n",
    "\n",
    "    for model_number in model_numbers:\n",
    "        model = df_models[df_models['model'] == model_number]\n",
    "        edges_score, confs_score = group.loc[group['model'] == model_number, 'edges_score'].values[0], \\\n",
    "                                   group.loc[group['model'] == model_number, 'confs_score'].values[0]\n",
    "        \n",
    "        for row in range(B.shape[0]):\n",
    "            for col in range(B.shape[1]):\n",
    "                if row != col:\n",
    "                    true_label_edge = model['edges'].values[0][row, col]\n",
    "                    true_label_conf = model['confs'].values[0][row, col]\n",
    "                    labels.append(true_label_edge)\n",
    "                    labels.append(true_label_conf)\n",
    "                    scores.append(edges_score[row, col])\n",
    "                    scores.append(confs_score[row, col])\n",
    "\n",
    "    # Calculate AUC ROC and add to results dataframe\n",
    "    auc_roc = roc_auc_score(labels, scores)\n",
    "    df_list.append(pd.DataFrame({'experiment': experiment, 'samplesize': samplesize, 'method': method,\n",
    "                              'model': model_number, 'auc_roc': auc_roc}, index=[0]))\n",
    "\n",
    "# Concatenate dataframes and reset index\n",
    "auc_asp = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb4de4",
   "metadata": {},
   "source": [
    "## Ratio of unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daee932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio of unknowns to the total number of features\n",
    "df_asp['ratio'] = (df_asp['pu_edge'] + df_asp['nu_edge'] + df_asp['pu_conf'] + df_asp['nu_conf']) / 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0761fa2",
   "metadata": {},
   "source": [
    "# Figures 1 - 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5bb7c",
   "metadata": {},
   "source": [
    "This section of code generates several plots and tables to compare the performance of different algorithms on different datasets and sample sizes.\n",
    "\n",
    "The first set of plots are point plots that show the ROC and accuracy scores for each algorithm, with the sample size on the x-axis. The plot is repeated four times, twice for ROC and twice for accuracy, with the sample sizes of 10,000 and inf shown. All algorithms are plotted on each plot, with the mean and standard deviation of the scores for each algorithm shown.\n",
    "\n",
    "The second set of plots are box plots that show the distribution of ROC and accuracy scores for each algorithm across different experiments and sample sizes. The plots are repeated twice, once for ROC and once for accuracy, with all algorithms included. Each plot also includes a table that shows the mean scores for each experiment and algorithm, and highlights the best performing algorithm in each experiment.\n",
    "\n",
    "Each of the plots in this section is accompanied by a table that provides additional information about the performance of the algorithms. We examine the performance of the algorithms when considering the combination of edges and confounders, as well as when considering only edges or only confounders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e6005",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b4437",
   "metadata": {},
   "source": [
    "### Fig. 1: Accuracies sample size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the plots\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "## Accuracy\n",
    "# Calculate the accuraries for this samplesize\n",
    "accuracies_asp = df_asp[['experiment', 'samplesize', 'method', 'acc_absent']].copy()\n",
    "accuracies_asp = accuracies_asp[accuracies_asp['samplesize'] == 1_000]\n",
    "accuracies_asp = accuracies_asp.rename(columns={'acc_absent': 'acc'})\n",
    "accuracies_asp['method'] = accuracies_asp['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "accuracies_llc = df_llc[['experiment', 'samplesize', 'method', 'acc_4']].copy()\n",
    "accuracies_llc = accuracies_llc[accuracies_llc['samplesize'] == 1_000]\n",
    "accuracies_llc = accuracies_llc.rename(columns={'acc_4': 'acc'})\n",
    "accuracies_llc['method'] = accuracies_llc['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'method': None, 'acc_absent': None}]\n",
    "accuracies = pd.concat([accuracies_llc, accuracies_asp, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "accuracies = accuracies[(accuracies['method'] != 'LLC-NF') | (accuracies['experiment'] != 0)]\n",
    "\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "# Plot\n",
    "sns.pointplot(data=accuracies, x='experiment', y='acc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Weak classifier\n",
    "plt.axhline(y=acc_weak_classifier, color='forestgreen', linestyle='--', label=\"Weak \\nClassifier\", \n",
    "            xmin=.05, xmax=.95)\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == 1_000]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20, labelpad=25)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "\n",
    "# ax.set_title('Accuracy of ASP-s, ASP-d, LLC-F, and LLC-NF for 1,000 samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116666c",
   "metadata": {},
   "source": [
    "### Fig. 2: Accuracies sample size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba82276",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the plots\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "## Accuracy\n",
    "# Calculate the accuraries for this samplesize\n",
    "accuracies_asp = df_asp[['experiment', 'samplesize', 'method', 'acc_absent']].copy()\n",
    "accuracies_asp = accuracies_asp[accuracies_asp['samplesize'] == 10_000]\n",
    "accuracies_asp = accuracies_asp.rename(columns={'acc_absent': 'acc'})\n",
    "accuracies_asp['method'] = accuracies_asp['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "accuracies_llc = df_llc[['experiment', 'samplesize', 'method', 'acc_4']].copy()\n",
    "accuracies_llc = accuracies_llc[accuracies_llc['samplesize'] == 10_000]\n",
    "accuracies_llc = accuracies_llc.rename(columns={'acc_4': 'acc'})\n",
    "accuracies_llc['method'] = accuracies_llc['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'method': None, 'acc_absent': None}]\n",
    "accuracies = pd.concat([accuracies_llc, accuracies_asp, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "accuracies = accuracies[(accuracies['method'] != 'LLC-NF') | (accuracies['experiment'] != 0)]\n",
    "\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "# Plot\n",
    "sns.pointplot(data=accuracies, x='experiment', y='acc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Weak classifier\n",
    "plt.axhline(y=acc_weak_classifier, color='forestgreen', linestyle='--', label=\"Weak \\nClassifier\", \n",
    "            xmin=.05, xmax=.95)\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == 10_000]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20, labelpad=25)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "\n",
    "# ax.set_title('Accuracy of ASP-s, ASP-d, LLC-F, and LLC-NF for 10,000 samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e7b65",
   "metadata": {},
   "source": [
    "### Fig. 3: Accuracies infinite sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d19bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the plots\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "## Accuracy\n",
    "# Calculate the accuraries for this samplesize\n",
    "accuracies_asp = df_asp[['experiment', 'samplesize', 'method', 'acc_absent']].copy()\n",
    "accuracies_asp = accuracies_asp[accuracies_asp['samplesize'] == np.inf]\n",
    "accuracies_asp = accuracies_asp.rename(columns={'acc_absent': 'acc'})\n",
    "accuracies_asp['method'] = accuracies_asp['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "accuracies_llc = df_llc[['experiment', 'samplesize', 'method', 'acc_4']].copy()\n",
    "accuracies_llc = accuracies_llc[accuracies_llc['samplesize'] == np.inf]\n",
    "accuracies_llc = accuracies_llc.rename(columns={'acc_4': 'acc'})\n",
    "accuracies_llc['method'] = accuracies_llc['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'method': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'method': None, 'acc_absent': None}]\n",
    "accuracies = pd.concat([accuracies_llc, accuracies_asp, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "accuracies = accuracies[(accuracies['method'] != 'LLC-NF') | (accuracies['experiment'] != 0)]\n",
    "\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "# Plot\n",
    "sns.pointplot(data=accuracies, x='experiment', y='acc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Weak classifier\n",
    "plt.axhline(y=acc_weak_classifier, color='forestgreen', linestyle='--', label=\"Weak \\nClassifier\", \n",
    "            xmin=.05, xmax=.95)\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == np.inf]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20, labelpad=25)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "\n",
    "# ax.set_title('Accuracy of ASP-s, ASP-d, LLC-F, and LLC-NF for infinite samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e01159",
   "metadata": {},
   "source": [
    "### Fig. 4: Average accuracies by dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dataframe\n",
    "df_asp_filtered = df_asp[['experiment', 'samplesize', 'method', 'acc_absent']].copy()\n",
    "df_asp_filtered = df_asp_filtered.rename(columns={'acc_absent': 'acc'})\n",
    "df_asp_filtered['method'] = df_asp_filtered['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "\n",
    "df_llc_filtered = df_llc[['experiment', 'samplesize', 'method', 'acc_4']].copy()\n",
    "df_llc_filtered = df_llc_filtered.rename(columns={'acc_4': 'acc'})\n",
    "df_llc_filtered['method'] = df_llc_filtered['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "df_new = pd.concat([df_asp_filtered, df_llc_filtered], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "df_new = df_new[(df_new['method'] != 'LLC-NF') | (df_new['experiment'] != 0)]\n",
    "\n",
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# Define custom color palette to be consistent across plots\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data using point plots\n",
    "sns.boxplot(data=df_new, x='samplesize', y='acc', hue='method', \n",
    "            hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], palette=colors)\n",
    "\n",
    "# Add a horizontal dotted green line\n",
    "plt.axhline(y=acc_weak_classifier, color='forestgreen', linestyle='--', label=\"Weak \\nClassifier\", \n",
    "#             xmin=.05, xmax=.9)\n",
    "            xmin=.01, xmax=.99)\n",
    "                                                                           \n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experiment', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20, labelpad=20)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "# ax.set_title('Accuracy of ASP-s, ASP-d, LLC-F, and LLC-NF across different experiments and sample sizes - edges and confounders.')\n",
    "\n",
    "# Set x-axis tick labels\n",
    "ax.set_xticklabels(['1e3', '1e4', '1e5', 'inf'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-.8, df_new['samplesize'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d00f5",
   "metadata": {},
   "source": [
    "## AUC ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60b4e9",
   "metadata": {},
   "source": [
    "### Fig. 5: AUC ROC sample size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dataframe\n",
    "df_asp_filtered = auc_asp[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_asp_filtered = df_asp_filtered[df_asp_filtered['samplesize'] == 1_000]\n",
    "df_asp_filtered['method'] = df_asp_filtered['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "\n",
    "df_llc_filtered = auc_llc[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_llc_filtered = df_llc_filtered[df_llc_filtered['samplesize'] == 1_000]\n",
    "df_llc_filtered['method'] = df_llc_filtered['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "\n",
    "# create a new row with experiment=1 and NaN values for other columns\n",
    "new_rows = [{'experiment': 1, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None}]\n",
    "\n",
    "df_new = pd.concat([df_asp_filtered, df_llc_filtered, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "df_new = df_new[(df_new['method'] != 'LLC-NF') | (df_new['experiment'] != 0)]\n",
    "\n",
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# Define custom color palette to be consistent across plots\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data using point plots\n",
    "sns.pointplot(data=df_new, x='experiment', y='auc_roc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == 1_000]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('AUC ROC', fontsize=20, labelpad=20)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "# ax.set_title('AUC ROC of ASP-s, ASP-d, LLC-F, and LLC-NF for 1,000 samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fac390",
   "metadata": {},
   "source": [
    "### Fig. 6: AUC ROC sample size 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dataframe\n",
    "df_asp_filtered = auc_asp[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_asp_filtered = df_asp_filtered[df_asp_filtered['samplesize'] == 10_000]\n",
    "df_asp_filtered['method'] = df_asp_filtered['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "\n",
    "df_llc_filtered = auc_llc[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_llc_filtered = df_llc_filtered[df_llc_filtered['samplesize'] == 10_000]\n",
    "df_llc_filtered['method'] = df_llc_filtered['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "\n",
    "# create a new row with experiment=1 and NaN values for other columns\n",
    "new_rows = [{'experiment': 1, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None}]\n",
    "\n",
    "df_new = pd.concat([df_asp_filtered, df_llc_filtered, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "df_new = df_new[(df_new['method'] != 'LLC-NF') | (df_new['experiment'] != 0)]\n",
    "\n",
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# Define custom color palette to be consistent across plots\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data using point plots\n",
    "sns.pointplot(data=df_new, x='experiment', y='auc_roc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == 10_000]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('AUC ROC', fontsize=20, labelpad=20)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "# ax.set_title('AUC ROC of ASP-s, ASP-d, LLC-F, and LLC-NF for 10,000 samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e4eb6",
   "metadata": {},
   "source": [
    "### Fig. 7: AUC ROC infinite sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f069d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dataframe\n",
    "df_asp_filtered = auc_asp[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_asp_filtered = df_asp_filtered[df_asp_filtered['samplesize'] == np.inf]\n",
    "df_asp_filtered['method'] = df_asp_filtered['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "\n",
    "df_llc_filtered = auc_llc[['experiment', 'samplesize', 'method', 'auc_roc']].copy()\n",
    "df_llc_filtered = df_llc_filtered[df_llc_filtered['samplesize'] == np.inf]\n",
    "df_llc_filtered['method'] = df_llc_filtered['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "\n",
    "# create a new row with experiment=1 and NaN values for other columns\n",
    "new_rows = [{'experiment': 1, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 16, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 26, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None},\n",
    "            {'experiment': 36, 'samplesize': None, 'method': None, 'model': None, 'acc_absent': None}]\n",
    "\n",
    "df_new = pd.concat([df_asp_filtered, df_llc_filtered, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "df_new = df_new[(df_new['method'] != 'LLC-NF') | (df_new['experiment'] != 0)]\n",
    "\n",
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# Define custom color palette to be consistent across plots\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data using point plots\n",
    "sns.pointplot(data=df_new, x='experiment', y='auc_roc', hue='method', hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], \n",
    "              palette=colors, markers=['o', 's', '^', 'v'], linestyles=['-', '--', ':', '-.'], dodge=.4, \n",
    "              estimator='mean', errorbar='sd')\n",
    "\n",
    "## Unknown ratios\n",
    "# Calculate the average ratio for this samplesize\n",
    "average_ratios = df_asp[['experiment', 'samplesize', 'method', 'ratio']].copy()\n",
    "average_ratios = average_ratios[average_ratios['samplesize'] == np.inf]\n",
    "average_ratios = average_ratios.groupby(['method', 'experiment'])['ratio'].mean()\n",
    "average_ratios = average_ratios.reset_index()[['method', 'experiment', 'ratio']]\n",
    "average_ratios['method'] = average_ratios['method'].replace({'asp_d': 'ASP-d \\nunknown ratio', 'asp_s': 'ASP-s \\nunknown ratio'})\n",
    "# Make the plot pretty and set colors\n",
    "new_rows = [{'experiment': 1, 'ratio': None, 'method': None},\n",
    "            {'experiment': 16, 'ratio': None, 'method': None},\n",
    "            {'experiment': 26, 'ratio': None, 'method': None},\n",
    "            {'experiment': 36, 'ratio': None, 'method': None}]\n",
    "colors = {'ASP-s \\nunknown ratio': '#ADD1E6', 'ASP-d \\nunknown ratio': '#3787C0'}\n",
    "average_ratios = pd.concat([average_ratios, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "# Plot\n",
    "sns.pointplot(data=average_ratios, x='experiment', y='ratio', hue='method', hue_order=['ASP-s \\nunknown ratio', 'ASP-d \\nunknown ratio'], \n",
    "              palette=colors, markers=['*', 'X'], linestyles=['-', '--'], dodge=.4)\n",
    "\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experimental Setup', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('AUC ROC', fontsize=20, labelpad=20)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "# ax.set_title('AUC ROC of ASP-s, ASP-d, LLC-F, and LLC-NF for infinte samples - edges and confounders.')\n",
    "ax.set_xticklabels(['0', '', '11', '12', '13', '14', '15',  '', \n",
    "                    '21', '22', '23', '24', '25', '', \n",
    "                    '31', '32', '33', '34', '35', '',\n",
    "                    '41', '42', '43', '44', '45'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-2.5, accuracies['experiment'].nunique() + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde0c95",
   "metadata": {},
   "source": [
    "### Fig. 8: Average AUC ROC by dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473897c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the combined dataframe\n",
    "df_asp_filtered = auc_asp\n",
    "df_asp_filtered['method'] = df_asp_filtered['method'].replace({'asp_s': 'ASP-s', 'asp_d': 'ASP-d'})\n",
    "df_llc_filtered = auc_llc\n",
    "df_llc_filtered['method'] = df_llc_filtered['method'].replace({'llc_f_2': 'LLC-F', 'llc_f_3': 'LLC-F',\n",
    "                                                      'llc_nf_2': 'LLC-NF', 'llc_nf_3': 'LLC-NF'})\n",
    "df_new = pd.concat([df_asp_filtered, df_llc_filtered], ignore_index=True)\n",
    "\n",
    "# Filter out the data point for 'LLC-NF' and experiment 0\n",
    "df_new = df_new[(df_new['method'] != 'LLC-NF') | (df_new['experiment'] != 0)]\n",
    "\n",
    "# Set plot size\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "\n",
    "# Define custom color palette to be consistent across plots\n",
    "colors = {'ASP-s': '#ADD1E6', 'ASP-d': '#3787C0', 'LLC-F': '#FDB97D', 'LLC-NF': '#E95F0E'}\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data using point plots\n",
    "sns.boxplot(data=df_new, x='samplesize', y='auc_roc', hue='method', \n",
    "            hue_order=['ASP-s', 'ASP-d', 'LLC-F', 'LLC-NF'], palette=colors)\n",
    "                                                                           \n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Experiment', fontsize=20, labelpad=20)\n",
    "ax.set_ylabel('Accuracy', fontsize=20, labelpad=20)\n",
    "ax.set_ylim(-0.01, 1.05)\n",
    "# ax.set_title('AUC ROC of ASP-s, ASP-d, LLC-F, and LLC-NF across different experiments and sample sizes - edges and confounders.')\n",
    "\n",
    "# Set x-axis tick labels\n",
    "ax.set_xticklabels(['1e3', '1e4', '1e5', 'inf'])\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Add legend\n",
    "legend = ax.legend(fontsize=16)\n",
    "legend.set_title('Method', prop={'size': 20})\n",
    "legend.set_bbox_to_anchor([1, 0.8])\n",
    "\n",
    "# Add padding to the plot on the right and left sides\n",
    "_=ax.set_xlim(-.8, df_new['samplesize'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e401b2",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec8f86",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">LLC plain algorithm input and outputs</p>\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\" rowspan=\"6\">parameters</td>\n",
    "      <td style=\"text-align:left\">penalty: str</td>\n",
    "      <td style=\"text-align:left\">Type of penalization/regularization used in the solving of the linear equation systems ('none', 'L0', 'L1' or 'L2').</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">lambda: int</td>\n",
    "      <td style=\"text-align:left\">Regularization parameter.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">null: bool</td>\n",
    "      <td style=\"text-align:left\">Specifies whether to request identifiability information about the null-space.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">rules: List[int]</td>\n",
    "      <td style=\"text-align:left\">Which of the which of the faithfulness rules (1,2,3) to apply.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">pc_significance: int</td>\n",
    "      <td style=\"text-align:left\">Constraints from the first faithfulness rule are calculated using the skeleton of the PC algorithm (Spirteset al., 1993), for which a significance level needs to be specified (default = 0.05).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">cond. indep. test</td>\n",
    "      <td style=\"text-align:left\">Constraints from faithfulness rules 2 and 3 are calculated using a conditional independence test and thresholds for dependence (p_dep) and independence (p_indep). A partial correlation test with p_dep = p_indep = 0.05 were originally used.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td  style=\"text-align:left\" rowspan=\"8\">output</td>\n",
    "      <td style=\"text-align:left\">B: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Estimate for the direct effects.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Ce: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Estimate of the covariance matrix. May contain NA if cov-condition is not satisfied for all pairs.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">PC: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Which pair conditions were satisfied by the experiments?</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">COV: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Which cov conditions were satisfied by the experiments? (symmetric matrix)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Bcon: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Conservative estimate of which elements of B are identified. See article for details.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Bnull: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Norms of the parameters projected to the null-space (only generated when null=True). Values close to 0 mean that the element of B is identified. See article for details.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Bvar: Matrix[float]</td>\n",
    "      <td style=\"text-align:left\">Posterior variances of the parameter estimates. More robust way of determining the identified parameters (only generated when null=True). Values close to 0 mean that the element of B is identified.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align:left\">Cecon: Matrix[bool]</td>\n",
    "      <td style=\"text-align:left\">Conservative estimate of which elements of Ce are identified.</td>\n",
    "    </tr>\n",
    " </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd060390",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; font-weight:bold;\">LLC bootstrap algorithm input and outputs</p>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">parameters <br> (additional <br> to plain)</td>\n",
    "    <td style=\"text-align:left\">n_bootstraps: int</td>\n",
    "    <td style=\"text-align:left\">Number times to run plain version of llc.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\" rowspan=\"4\">output <br> (additional <br> to plain)</td>\n",
    "    <td style=\"text-align:left\">Bsd: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Standard deviation of the estimated direct effects over the bootstraps.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Cesd: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Standard deviations of estimated elements of the covariance matrix.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Bz: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Z-scores for the elements of B. Absolute Z score is a measure of confidence on the existence of the edgs.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">Cez: Matrix[float]</td>\n",
    "    <td style=\"text-align:left\">Z-scores for the elements of Ce. Absolute Z score is a measure of confidence on the existence of the confounders.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\" rowspan=\"2\">possible <br> usages of <br> output</td>\n",
    "    <td style=\"text-align:left\">predict {absent, present, [unknown]}</td>\n",
    "    <td style=\"text-align:left\">Classify as present if corresponding absolute value in Bz (or Cez for confounders) &gt; threshold. Classify as absent if corresponding absolute value in Bz (or Cez) &lt;= threshold. Use a threshold of 5 for both edges and confounders. Optional: classify as unknown if corresponding element in Bcon (or Cecon) is False, features classified as present or absent will have a value of True in Bcon (or Cecon).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:left\">confidence value <br> (for ROC)</td>\n",
    "    <td style=\"text-align:left\">Absolute value of Bz (Cez for confounders).</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
